<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="The GBM developers" />

<meta name="date" content="2024-01-21" />

<title>Generalized Boosted Models: A guide to the gbm package</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>







<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Generalized Boosted Models: A guide to the
gbm package</h1>
<h4 class="author">The GBM developers</h4>
<h4 class="date">January 21, 2024</h4>



<p>Boosting takes on various forms with different programs using
different loss functions, different base models, and different
optimization schemes. The gbm3 package takes the approach described in
Friedman (2001) and Friedman (2002). Some of the terminology differs,
mostly due to an effort to cast boosting terms into more standard
statistical terminology (e.g. deviance). In addition, the gbm3 package
implements boosting for models commonly used in statistics but not
commonly associated with boosting. The Cox proportional hazard model,
for example, is an incredibly useful model and the boosting framework
applies quite readily with only slight modification Ridgeway (1999).
Also some algorithms implemented in the gbm3 package differ from the
standard implementation. The AdaBoost algorithm (Freund and Schapire,
1997) has a particular loss function and a particular optimization
algorithm associated with it. The gbm3 implementation of AdaBoost adopts
AdaBoost’s exponential loss function (its bound on misclassification
rate) but uses Friedman’s gradient descent algorithm rather than the
original one proposed. So the main purposes of this document is to spell
out in detail what the gbm3 package implements.</p>
<p><strong>NB:</strong> This document refers heavily to the
<code>gbm</code> function. This function is part of the original R API
provided by the package; while this API is still functional, as of
Version 2.1.06 the package contains an updated API which performs
boosting: <code>gbmt</code>. For more details on how to use the new API,
see the “Getting started with the gbm package” vignette.</p>
<div id="high-level-description-of-stochastic-gradient-boosting" class="section level1">
<h1>High-level description of stochastic gradient boosting</h1>
<p>This section provides a high-level description of stochastic gradient
boosting; all of the concepts and formulas touched upon here are
discussed in much greater detail further on in this document.</p>
<p>Gradient boosting generates a low bias predictor by combining a lot
of high bias weak learners in an additive fashion. More formally,
gradient boosting approximates the function <span class="math inline">\(f(\textbf{x})\)</span>, which maps covariates to
the response variable <em>y</em>, as follows: <span class="math display">\[\hat{f}(\mathbf{x}) = \hat f_{0} + \lambda
\sum_{i=1}^{T} \hat{f}_{i}(\mathbf{x}).\qquad (1)\]</span> The number of
weak learners to fit is denoted by <em>T</em>. <span class="math inline">\(\hat{f}_0\)</span> is an initial constant. The
shrinkage parameter is denoted by <span class="math inline">\(\lambda\)</span>. The shrinkage parameter acts as
a regularization to prevent overfitting of the model. However, while a
smaller shrinkage parameter will tend to prevent overfitting it will
require a larger the number of iterations <em>T</em> to achieve a
desired performance. This trade-off is discussed further in the “Getting
started with gbm” vignette and in more detail later in this
document.</p>
<p>In the <code>gbm3</code> package the weak learners are regression
trees fitted to predict the residuals (or error) of the current fit from
the observations’ features. The process of subsampling the training set,
known as bagging, and fitting the decision tree to the subsample has the
effect of reducing the variance in the estimate: <span class="math inline">\(\hat{f}(\mathbf{x})\)</span>. The covariate data
is subsampled on a by-observation basis, that is observations are
subsampled and the covariate data associated with these observations
appear in the subsample. The residuals to which the trees are fitted are
calculated via: <span class="math display">\[z_{i} =
-\frac{\partial}{\partial
f(\mathbf{x}_i)}\Psi(y_{i},f(\mathbf{x}_i))|_{f(\mathbf{x}_i) =
\hat{f}(\mathbf{x}_i)} \qquad (2)\]</span> where <span class="math inline">\(\Psi(y_i, f(\mathbf{x}_i))\)</span> is the loss
function (usually the negative of the log likelihood) for the model.
This loss function depends on the observation’s actual response <span class="math inline">\(y_i\)</span> and the mapping function’s predicted
response <span class="math inline">\(f(\mathbf{x}_i)\)</span> (note in
(2) the function is evaluated as the model estimate at the current
boosting iteration).</p>
<p>Now the algorithm uses the shrinkage parameter to mitigate against
overfitting during boosting. However, another consideration that must be
tackled is whether or not the underlying weak learners are individually
overfitting to the residuals, <span class="math inline">\(z_i\)</span>.
If this is the case the performance of the model on new data will
greatly decrease. The algorithm stops this scenario from occurring by
restricting the maximum depth each decision tree can take.</p>
<p>The idea behind this is that boosted trees are decomposable in terms
of a “functional ANOVA decomposition”. Simply put, the deeper a tree is
the higher the order of the interactions between covariates the tree
will model, i.e. a “stump” will only model 1st order effects and thus
the model will be an additive boosted model. From this intuition, fixing
the maximum depth of a tree to a sensible low order value (generally a
depth of ~2-5 will suffice) reduces the complexity of fitted weak
learner and reduces the possibility of overfitting.</p>
<p>Returning to the main algorithm, by fitting decision trees to the
residuals, the boosted model’s training errors are reduced as more and
more weak learners are fit, while the threat of overfitting is
counteracted via the bagging, shrinkage, and truncation of the
individual trees. With the tree fit, the optimal terminal node
predictions may be calculated via: <span class="math display">\[\rho_k =
\arg \min_{\rho} \sum_{\mathbf{x}_i \in S_k} \Psi(y_i,
\hat{f}(\mathbf{x}_i) +\rho), \qquad (3)\]</span></p>
<p>where <span class="math inline">\(S_{k}\)</span> is the set of
covariate data “in” the terminal node <em>k</em> and <span class="math inline">\(\rho_k\)</span> is its optimal prediction. The
optimal predictions then provide the update to the additive sum defined
in (1), i.e. <span class="math inline">\(\hat{f}_i(\mathbf{x}) = \sum_k
\rho_{k(\mathbf{x})}\)</span>, where <span class="math inline">\(k(\mathbf{x})\)</span> indicates in which terminal
node an observation with features <span class="math inline">\(\mathbf{x}\)</span> would land. If the loss
associated with a node is independent of the data defining other
terminal nodes in the tree, the optimal predictions can be calculated
using simple line searching algorithms and sometimes can be analytically
derived. These terminal node predictions determine the predictions made
by an individual tree when provided with new covariate data and thus the
total boosted model predictions through (1).</p>
<p>To conclude, it should be noted that the residuals in Equation (2) do
not depend explicitly on the covariates <span class="math inline">\(\mathbf{x}_i\)</span>. The residuals capture the
error in the current estimate of the response variable and so the only
role the covariates play, until Equation (3), is in determining the
splits in the tree. This “degree of freedom” is exploited in the
implementation of the Cox proportional hazards model, see the “Guide to
the Cox Proportional Hazards model” vignettes for more details.</p>
</div>
<div id="gradient-boosting-in-more-detail" class="section level1">
<h1>Gradient boosting in more detail</h1>
<p>This section essentially presents the derivation of boosting
described in Friedman (2001). The <code>gbm3</code> package also adopts
the stochastic gradient boosting strategy, a small but important
enhancement on the basic algorithm, described in Friedman (2002).</p>
<div id="friedmans-gradient-boosting-machine" class="section level2">
<h2>Friedman’s gradient boosting machine</h2>
<p><strong>Friedman’s Gradient Boost algorithm</strong></p>
<p>Initialize <span class="math inline">\(\hat f(\mathbf{x})\)</span> to
be a constant, <span class="math inline">\(\hat f_0 = \arg \min_{\rho}
\sum_{i=1}^N \Psi(y_i,\rho)\)</span>. \ For <span class="math inline">\(t\)</span> in <span class="math inline">\(1,\ldots,T\)</span> do</p>
<ol style="list-style-type: decimal">
<li><p>Compute the negative gradient as the working response <span class="math display">\[z_i = -\left.\frac{\partial}{\partial
f(\mathbf{x}_i)} \Psi(y_i,f(\mathbf{x}_i)) \right|_{f(\mathbf{x}_i)=\hat
f(\mathbf{x}_i)}\]</span></p></li>
<li><p>Fit a regression model, <span class="math inline">\(g(\mathbf{x})\)</span>, predicting <span class="math inline">\(z_i\)</span> from the covariates <span class="math inline">\(\mathbf{x}_i\)</span>.</p></li>
<li><p>Choose a gradient descent step size as <span class="math display">\[\rho = \arg \min_{\rho} \sum_{i=1}^N
\Psi(y_i,\hat f(\mathbf{x}_i)+\rho g(\mathbf{x}_i))\]</span></p></li>
<li><p>Update the estimate of <span class="math inline">\(f(\mathbf{x})\)</span> as <span class="math display">\[\hat f(\mathbf{x}) \leftarrow \hat f(\mathbf{x})
+ \rho g(\mathbf{x})\]</span></p></li>
</ol>
<p>Friedman (2001) and the companion paper Friedman (2002) extended the
work of Friedman, Hastie, and Tibshirani (2000) and laid the ground work
for a new generation of boosting algorithms. Using the connection
between boosting and optimization, this work proposes the Gradient
Boosting Machine.</p>
<p>In any function estimation problem we wish to find a regression
function, <span class="math inline">\(\hat f(\mathbf{x})\)</span>, that
minimizes the expectation of some loss function, <span class="math inline">\(\Psi(y,f)\)</span>, as shown in (4).</p>
<p><span class="math display">\[
\begin{aligned}
\hspace{0.5in}
\hat f(\mathbf{x}) &amp;= \arg \min_{f(\mathbf{x})} E_{y,\mathbf{x}}
\Psi(y,f(\mathbf{x})) \qquad\qquad (4) \\ % NonparametricRegression1
&amp;= \arg \min_{f(\mathbf{x})} E_x \left[ E_{y|\mathbf{x}}
\Psi(y,f(\mathbf{x})) \Big| \mathbf{x} \right]
\end{aligned}
\]</span></p>
<p>We will focus on finding estimates of <span class="math inline">\(f(\mathbf{x})\)</span> such that
<!-- NonparametricRegression2 --> <span class="math display">\[\hat
f(\mathbf{x}) = \arg \min_{f(\mathbf{x})} E_{y|\mathbf{x}} \left[
\Psi(y,f(\mathbf{x}))|\mathbf{x} \right]\qquad (5)\]</span> Parametric
regression models assume that <span class="math inline">\(f(\mathbf{x})\)</span> is a function with a finite
number of parameters, <span class="math inline">\(\beta\)</span>, and
estimates them by selecting those values that minimize a loss function
(e.g. squared error loss) over a training sample of <span class="math inline">\(N\)</span> observations on <span class="math inline">\((y,\mathbf{x})\)</span> pairs as in (6).
<!-- {eq:Friedman1} --> <span class="math display">\[\hat\beta = \arg
\min_{\beta} \sum_{i=1}^N \Psi(y_i,f(\mathbf{x}_i;\beta)) \qquad
(6)\]</span> When we wish to estimate <span class="math inline">\(f(\mathbf{x})\)</span> non-parametrically the task
becomes more difficult. Again we can proceed similarly to Friedman,
Hastie, and Tibshirani (2000) and modify our current estimate of <span class="math inline">\(f(\mathbf{x})\)</span> by adding a new function
<span class="math inline">\(f(\mathbf{x})\)</span> in a greedy fashion.
Letting <span class="math inline">\(f_i = f(\mathbf{x}_i)\)</span>, we
see that we want to decrease the <span class="math inline">\(N\)</span>
dimensional function <!-- {EQ:Friedman2} --> <span class="math display">\[\begin{aligned}
J(\mathbf{f}) &amp;= \sum_{i=1}^N \Psi(y_i,f(\mathbf{x}_i)) \nonumber \\
                          &amp;= \sum_{i=1}^N \Psi(y_i,F_i).
\end{aligned}\]</span> The negative gradient of <span class="math inline">\(J(\mathbf{f})\)</span> indicates the direction of
the locally greatest decrease in <span class="math inline">\(J(\mathbf{f})\)</span>. Gradient descent would
then have us modify <span class="math inline">\(\mathbf{f}\)</span> as
<!-- eq:Friedman3 --></p>
<p><span class="math display">\[\mathbf{\hat f} \leftarrow \mathbf{\hat
f} - \rho \nabla J(\mathbf{f}) \qquad (7)\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is the size of the
step along the direction of greatest descent. Clearly, this step alone
is far from our desired goal. First, it only fits <span class="math inline">\(f\)</span> at values of <span class="math inline">\(\mathbf{x}\)</span> for which we have
observations. Second, it does not take into account that observations
with similar <span class="math inline">\(\mathbf{x}\)</span> are likely
to have similar values of <span class="math inline">\(f(\mathbf{x})\)</span>. Both these problems would
have disastrous effects on generalization error. However, Friedman
suggests selecting a class of functions that use the covariate
information to approximate the gradient, usually a regression tree. This
line of reasoning produces his Gradient Boosting algorithm shown at the
top of this section. At each iteration the algorithm determines the
direction, the gradient, in which it needs to improve the fit to the
data and selects a particular model from the allowable class of
functions that is in most agreement with the direction. In the case of
squared-error loss, <span class="math inline">\(\Psi(y_i,f(\mathbf{x}_i)) = \sum_{i=1}^N
(y_i-f(\mathbf{x}_i))^2\)</span>, this algorithm corresponds exactly to
residual fitting.</p>
<p>There are various ways to extend and improve upon the basic framework
suggested in the Gradient Boosting algorithm. For example, Friedman
(2001) substituted several choices for <span class="math inline">\(\Psi\)</span> to develop new boosting algorithms
for robust regression with least absolute deviation and Huber loss
functions. Friedman (2002) showed that a simple subsampling trick can
greatly improve predictive performance while simultaneously reduce
computation time. This guide discusses some of these modifications
next.</p>
</div>
</div>
<div id="improving-boosting-methods-using-control-of-the-learning-rate-sub-sampling-and-a-decomposition-for-interpretation" class="section level1">
<h1>Improving boosting methods using control of the learning rate,
sub-sampling, and a decomposition for interpretation</h1>
<p>This section explores the variations of the previous algorithms that
have the potential to improve their predictive performance and
interpretability. In particular, by controlling the optimization speed
or learning rate, introducing low-variance regression methods, and
applying ideas from robust regression we can produce non-parametric
regression procedures with many desirable properties. As a by-product
some of these modifications lead directly into implementations for
learning from massive datasets. All these methods take advantage of the
general form of boosting <span class="math display">\[\hat f(\mathbf{x})
\leftarrow \hat f(\mathbf{x}) + E(z(y,\hat f(\mathbf{x}))|\mathbf{x})
\qquad (7)\]</span>.</p>
<p>So far we have taken advantage of this form only by substituting in
our favorite regression procedure for <span class="math inline">\(E_w(z|\mathbf{x})\)</span>. I will discuss some
modifications to estimating <span class="math inline">\(E_w(z|\mathbf{x})\)</span> that have the potential
to improve our algorithm.</p>
<div id="decreasing-the-learning-rate" class="section level2">
<h2>Decreasing the learning rate</h2>
<p>As several authors have phrased slightly differently, “…boosting,
whatever flavor, seldom seems to overfit, no matter how many terms are
included in the additive expansion”. This is not true as the discussion
to Friedman, Hastie, and Tibshirani (2000) points out.</p>
<p>In the update step of any boosting algorithm we can introduce a
learning rate to dampen the proposed move.
<!-- \label{eq:shrinkage}  --> <span class="math display">\[\hat
f(\mathbf{x}) \leftarrow \hat f(\mathbf{x}) + \lambda E(z(y,\hat
f(\mathbf{x}))|\mathbf{x}) \qquad (8)\]</span>.</p>
<p>By multiplying the gradient step by <span class="math inline">\(\lambda\)</span> as in (8) we have control on the
rate at which the boosting algorithm descends the error surface (or
ascends the likelihood surface). When <span class="math inline">\(\lambda=1\)</span> we return to performing full
gradient steps. Friedman (2001) relates the learning rate to
regularization through shrinkage.</p>
<p>The optimal number of iterations, <span class="math inline">\(T\)</span>, and the learning rate, <span class="math inline">\(\lambda\)</span>, depend on each other. In
practice I set <span class="math inline">\(\lambda\)</span> to be as
small as possible and then select <span class="math inline">\(T\)</span>
by cross-validation. Performance is best when <span class="math inline">\(\lambda\)</span> is as small as possible, with
decreasing marginal utility for smaller and smaller <span class="math inline">\(\lambda\)</span>. Slower learning rates do not
necessarily scale the number of optimal iterations. That is, when <span class="math inline">\(\lambda=1.0\)</span> and the optimal <span class="math inline">\(T\)</span> is 100 iterations, it does <em>not</em>
necessarily imply that when <span class="math inline">\(\lambda=0.1\)</span> the optimal <span class="math inline">\(T\)</span> is 1000 iterations.</p>
</div>
<div id="variance-reduction-using-subsampling" class="section level2">
<h2>Variance reduction using subsampling</h2>
<p>Friedman (2002) proposed the stochastic gradient boosting algorithm
that simply samples uniformly without replacement from the dataset
before estimating the next gradient step. He found that this additional
step greatly improved performance. We estimate the regression <span class="math inline">\(E(z(y,\hat f(\mathbf{x}))|\mathbf{x})\)</span>
using a random subsample of the dataset.</p>
</div>
<div id="anova-decomposition" class="section level2">
<h2>ANOVA decomposition</h2>
<p>Certain function approximation methods are decomposable in terms of a
“functional ANOVA decomposition”. That is a function is decomposable as
<!-- \label{ANOVAdecomp}  --> <span class="math display">\[f(\mathbf{x})
= \sum_j f_j(x_j) + \sum_{jk} f_{jk}(x_j,x_k) + \sum_{jk\ell}
f_{jk\ell}(x_j,x_k,x_\ell) + \cdots \qquad (9)\]</span> .</p>
<p>This applies to boosted trees. Regression stumps (one split decision
trees) depend on only one variable and fall into the first term of (9).
Trees with two splits fall into the second term of (9) and so on. By
restricting the depth of the trees produced on each boosting iteration
we can control the order of approximation. Often additive components are
sufficient to approximate a multivariate function well, generalized
additive models, the naïve Bayes classifier, and boosted stumps for
example. When the approximation is restricted to a first order we can
also produce plots of <span class="math inline">\(x_j\)</span> versus
<span class="math inline">\(f_j(x_j)\)</span> to demonstrate how changes
in <span class="math inline">\(x_j\)</span> might affect changes in the
response variable.</p>
</div>
<div id="relative-influence" class="section level2">
<h2>Relative influence</h2>
<p>Friedman (2001) also develops an extension of a variable’s “relative
influence” for boosted estimates. For tree based methods the approximate
relative influence of a variable <span class="math inline">\(x_j\)</span> is <!-- \label{RelInfluence}  -->
<span class="math display">\[\hat J_j^2 =
\hspace{-0.1in}\sum_{\mathrm{splits~on~}x_j}\hspace{-0.2in}I_t^2\]</span></p>
<p>where <span class="math inline">\(I_t^2\)</span> is the empirical
improvement by splitting on <span class="math inline">\(x_j\)</span> at
that point. Friedman’s extension to boosted models is to average the
relative influence of variable <span class="math inline">\(x_j\)</span>
across all the trees generated by the boosting algorithm.</p>
<p><strong>Boosting as implemented in <code>gbm()</code></strong></p>
<p>Select</p>
<ol style="list-style-type: decimal">
<li><p>a loss function (<code>distribution</code>)</p></li>
<li><p>the number of iterations, <span class="math inline">\(T\)</span>
(<code>num_trees</code>)</p></li>
<li><p>the depth of each tree, <span class="math inline">\(K\)</span>
(<code>interaction_depth</code>)</p></li>
<li><p>the shrinkage (or learning rate) parameter, <span class="math inline">\(\lambda\)</span> (<code>shrinkage</code>)</p></li>
<li><p>the subsampling rate, <span class="math inline">\(p\)</span>
(<code>bag_fraction</code>)</p></li>
</ol>
<p>Initialize <span class="math inline">\(\hat f(\mathbf{x})\)</span> to
be a constant, <span class="math inline">\(\hat f(\mathbf{x}) = \arg
\min_{\rho} \sum_{i=1}^N \Psi(y_i,\rho)\)</span></p>
<p>For <span class="math inline">\(t\)</span> in <span class="math inline">\(1,\ldots,T\)</span> do</p>
<ol style="list-style-type: decimal">
<li><p>Compute the negative gradient as the working response <span class="math display">\[z_i = -\left.\frac{\partial}{\partial
f(\mathbf{x}_i)} \Psi(y_i,f(\mathbf{x}_i)) \right|_{f(\mathbf{x}_i)=\hat
f(\mathbf{x}_i)}\]</span></p></li>
<li><p>Randomly select <span class="math inline">\(p\times N\)</span>
cases from the dataset</p></li>
<li><p>Fit a regression tree with <span class="math inline">\(K\)</span>
terminal nodes, <span class="math inline">\(g(\mathbf{x})=E(z|\mathbf{x})\)</span>. This tree
is fit using only those randomly selected observations</p></li>
<li><p>Compute the optimal terminal node predictions, <span class="math inline">\(\rho_1,\ldots,\rho_K\)</span>, as <span class="math display">\[\rho_k = \arg \min_{\rho} \sum_{\mathbf{x}_i\in
S_k} \Psi(y_i,\hat f(\mathbf{x}_i)+\rho)\]</span> where <span class="math inline">\(S_k\)</span> is the set of <span class="math inline">\(\mathbf{x}\)</span>s that define terminal node
<span class="math inline">\(k\)</span>. Again this step uses only the
randomly selected observations.</p></li>
<li><p>Update <span class="math inline">\(\hat f(\mathbf{x})\)</span> as
<span class="math display">\[\hat f(\mathbf{x}) \leftarrow \hat
f(\mathbf{x}) + \lambda\rho_{k(\mathbf{x})}\]</span> where <span class="math inline">\(k(\mathbf{x})\)</span> gives the index of the
terminal node into which an observation with features <span class="math inline">\(\mathbf{x}\)</span> would fall.</p></li>
</ol>
</div>
</div>
<div id="common-user-options" class="section level1">
<h1>Common user options</h1>
<p>This section discusses the options to gbm that most users will need
to change or tune.</p>
<div id="loss-function" class="section level2">
<h2>Loss function</h2>
<p>The first and foremost choice is <code>distribution</code>. This
should be easily dictated by the application. For most classification
problems either <code>bernoulli</code> or <code>adaboost</code> will be
appropriate, the former being recommended. For continuous outcomes the
choices are <code>gaussian</code> (for minimizing squared error),
<code>laplace</code> (for minimizing absolute error), and quantile
regression (for estimating percentiles of the conditional distribution
of the outcome). Censored survival outcomes should require
<code>coxph</code>. Count outcomes may use <code>poisson</code> although
one might also consider <code>gaussian</code> or <code>laplace</code>
depending on the analytical goals.</p>
</div>
<div id="the-relationship-between-shrinkage-and-number-of-iterations" class="section level2">
<h2>The relationship between shrinkage and number of iterations</h2>
<p>The issues that most new users of gbm struggle with are the choice of
<code>num_trees</code> and <code>shrinkage</code>. It is important to
know that smaller values of <code>shrinkage</code> (almost) always give
improved predictive performance. That is, setting
<code>shrinkage=0.001</code> will almost certainly result in a model
with better out-of-sample predictive performance than setting
<code>shrinkage=0.01</code>. However, there are computational costs,
both storage and CPU time, associated with setting
<code>shrinkage</code> to be low. The model with
<code>shrinkage=0.001</code> will likely require ten times as many
iterations as the model with <code>shrinkage=0.01</code>, increasing
storage and computation time by a factor of 10. Figure 1 shows the
relationship between predictive performance, the number of iterations,
and the shrinkage parameter. Note that the increase in the optimal
number of iterations between two choices for shrinkage is roughly equal
to the ratio of the shrinkage parameters. It is generally the case that
for small shrinkage parameters, 0.001 for example, there is a fairly
long plateau in which predictive performance is at its best. My rule of
thumb is to set <code>shrinkage</code> as small as possible while still
being able to fit the model in a reasonable amount of time and storage.
I usually aim for 3,000 to 10,000 iterations with shrinkage rates
between 0.01 and 0.001.</p>
<div class="figure">
<img src="data:image/png;base64,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" alt="**Figure 1**. Out-of-sample predictive performance by number of iterations and shrinkage. Smaller values of the shrinkage parameter offer improved predictive performance, but with decreasing marginal improvement" width="100%" />
<p class="caption">
<strong>Figure 1</strong>. Out-of-sample predictive performance by
number of iterations and shrinkage. Smaller values of the shrinkage
parameter offer improved predictive performance, but with decreasing
marginal improvement
</p>
</div>
</div>
<div id="estimating-the-optimal-number-of-iterations" class="section level2">
<h2>Estimating the optimal number of iterations</h2>
<p>gbm3 offers three methods for estimating the optimal number of
iterations after the gbm model has been fit, an independent test set
(<code>test</code>), out-of-bag estimation (<code>OOB</code>), and <span class="math inline">\(v\)</span>-fold cross validation
(<code>cv</code>). The function <code>gbm_perf</code> computes the
iteration estimate.</p>
<p>Like Friedman’s MART software, the independent test set method uses a
single holdout test set to select the optimal number of iterations. If
<code>train_fraction</code> is set to be less than 1, then only the
<em>first</em> <code>train_fraction</code><span class="math inline">\(\times\)</span><code>number of observations</code>
will be used to fit the model. Note that if the data are sorted in a
systematic way (such as cases for which <em>y</em>=1 come first), then
the data should be shuffled before running gbm. Those observations not
used in the model fit can be used to get an unbiased estimate of the
optimal number of iterations. The downside of this method is that a
considerable number of observations are used to estimate the single
regularization parameter (number of iterations) leaving a reduced
dataset for estimating the entire multivariate model structure. Use
<code>gbm_perf(...,method=&quot;test&quot;)</code> to obtain an estimate of the
optimal number of iterations using the held out test set.</p>
<p>If <code>bag_fraction</code> is set to be greater than 0 (0.5 is
recommended), gbm computes an out-of-bag estimate of the improvement in
predictive performance. It evaluates the reduction in deviance on those
observations not used in selecting the next regression tree. The
out-of-bag estimator underestimates the reduction in deviance. As a
result, it almost always is too conservative in its selection for the
optimal number of iterations. The motivation behind this method was to
avoid having to set aside a large independent dataset, which reduces the
information available for learning the model structure. Use
<code>gbm_perf(...,method=&quot;OOB&quot;)</code> to obtain the OOB estimate.</p>
<p>Lastly, gbm offers <span class="math inline">\(v\)</span>-fold cross
validation for estimating the optimal number of iterations. If when
fitting the gbm model, <code>cv_folds=5</code> then gbm will do 5-fold
cross validation. gbm will fit five gbm models in order to compute the
cross validation error estimate and then will fit a sixth and final gbm
model with <code>num_trees</code> iterations using all of the data. The
returned model object will have a component labeled
<code>cv_error</code>. Note that <code>gbm.more</code> will do
additional gbm iterations but will not add to the <code>cv_error</code>
component. Use <code>gbm_perf(...,method=&quot;cv&quot;)</code> to obtain the
cross validation estimate.</p>
<div class="figure" style="text-align: center">
<img src="data:image/png;base64,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" alt="**Figure 2**. Out-of-sample predictive performance of four methods of selecting the optimal number of iterations. The vertical axis plots performance relative the best. The boxplots indicate relative performance across thirteen real datasets from the UCI repository. See `demo(OOB-reps)`." width="80%" />
<p class="caption">
<strong>Figure 2</strong>. Out-of-sample predictive performance of four
methods of selecting the optimal number of iterations. The vertical axis
plots performance relative the best. The boxplots indicate relative
performance across thirteen real datasets from the UCI repository. See
<code>demo(OOB-reps)</code>.
</p>
</div>
<p>Figure 2 compares the three methods for estimating the optimal number
of iterations across 13 datasets. The boxplots show the methods
performance relative to the best method on that dataset. For most
datasets the method perform similarly, however, 5-fold cross validation
is consistently the best of them. OOB, using a 33% test set, and using a
20% test set all have datasets for which the perform considerably worse
than the best method. My recommendation is to use 5- or 10-fold cross
validation if you can afford the computing time. Otherwise you may
choose among the other options, knowing that OOB is conservative.</p>
</div>
</div>
<div id="available-distributions" class="section level1">
<h1>Available distributions</h1>
<p>This section gives some of the mathematical detail for each of the
distribution options that gbm offers. The gbm engine written in C++ has
access to a C++ class for each of these distributions. Each class
contains methods for computing the associated deviance, initial value,
the gradient, and the constants to predict in each terminal node.</p>
<p>In the equations shown below, for non-zero offset terms, replace
<span class="math inline">\(f(\mathbf{x}_i)\)</span> with <span class="math inline">\(o_i + f(\mathbf{x}_i)\)</span>.</p>
<div id="gaussian" class="section level2">
<h2>Gaussian</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td><span class="math inline">\(\displaystyle \frac{1}{\sum w_i} \sum
w_i(y_i-f(\mathbf{x}_i))^2\)</span></td>
</tr>
<tr class="even">
<td align="left">Initial value</td>
<td><span class="math inline">\(\displaystyle \frac{\sum
w_i(y_i-o_i)}{\sum w_i}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gradient</td>
<td><span class="math inline">\(z_i=y_i - f(\mathbf{x}_i)\)</span></td>
</tr>
<tr class="even">
<td align="left">Terminal node estimates</td>
<td><span class="math inline">\(\displaystyle \frac{\sum
w_i(y_i-f(\mathbf{x}_i))}{\sum w_i}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="adaboost" class="section level2">
<h2>AdaBoost</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td><span class="math inline">\(\displaystyle \frac{1}{\sum w_i} \sum
w_i\exp(-(2y_i-1)f(\mathbf{x}_i))\)</span></td>
</tr>
<tr class="even">
<td align="left">Initial value</td>
<td><span class="math inline">\(\displaystyle \frac{1}{2}\log\frac{\sum
y_iw_ie^{-o_i}}{\sum (1-y_i)w_ie^{o_i}}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gradient</td>
<td><span class="math inline">\(\displaystyle z_i=
-(2y_i-1)\exp(-(2y_i-1)f(\mathbf{x}_i))\)</span></td>
</tr>
<tr class="even">
<td align="left">Terminal node estimates</td>
<td><span class="math inline">\(\displaystyle \frac{\sum
(2y_i-1)w_i\exp(-(2y_i-1)f(\mathbf{x}_i))} {\sum
w_i\exp(-(2y_i-1)f(\mathbf{x}_i))}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="bernoulli" class="section level2">
<h2>Bernoulli</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td><span class="math inline">\(\displaystyle -2\frac{1}{\sum w_i} \sum
w_i(y_if(\mathbf{x}_i)-\log(1+\exp(f(\mathbf{x}_i))))\)</span></td>
</tr>
<tr class="even">
<td align="left">Initial value</td>
<td><span class="math inline">\(\displaystyle \log\frac{\sum
w_iy_i}{\sum w_i(1-y_i)}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gradient</td>
<td><span class="math inline">\(\displaystyle
z_i=y_i-\frac{1}{1+\exp(-f(\mathbf{x}_i))}\)</span></td>
</tr>
<tr class="even">
<td align="left">Terminal node estimates</td>
<td><span class="math inline">\(\displaystyle \frac{\sum
w_i(y_i-p_i)}{\sum w_ip_i(1-p_i)}\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td>where <span class="math inline">\(\displaystyle p_i =
\frac{1}{1+\exp(-f(\mathbf{x}_i))}\)</span></td>
</tr>
</tbody>
</table>
<p>Notes: For non-zero offset terms, the computation of the initial
value requires Newton-Raphson. Initialize <span class="math inline">\(f_0=0\)</span> and iterate <span class="math inline">\(\displaystyle f_0 \leftarrow f_0 + \frac{\sum
w_i(y_i-p_i)}{\sum w_ip_i(1-p_i)}\)</span> where <span class="math inline">\(\displaystyle p_i =
\frac{1}{1+\exp(-(o_i+f_0))}\)</span>.</p>
</div>
<div id="laplace" class="section level2">
<h2>Laplace</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td align="left"><span class="math inline">\(\frac{1}{\sum w_i} \sum
w_i|y_i-f(\mathbf{x}_i)|\)</span></td>
</tr>
<tr class="even">
<td align="left">Initial value</td>
<td align="left"><span class="math inline">\(\mathrm{median}_w(y)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gradient</td>
<td align="left"><span class="math inline">\(z_i=\mbox{sign}(y_i-f(\mathbf{x}_i))\)</span></td>
</tr>
<tr class="even">
<td align="left">Terminal node estimates</td>
<td align="left"><span class="math inline">\(\mathrm{median}_w(z)\)</span></td>
</tr>
</tbody>
</table>
<p>Notes: <span class="math inline">\(\mathrm{median}_w(y)\)</span>
denotes the weighted median, defined as the solution to the equation
<span class="math inline">\(\sum w_iI(y_i\leq m)=\frac{1}{2}\sum
w_i\)</span></p>
</div>
<div id="quantile-regression" class="section level2">
<h2>Quantile regression</h2>
<p>Contributed by Brian Kriegler (Kriegler, 2010).</p>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td><span class="math inline">\(\frac{1}{\sum w_i}
\left(\alpha\sum_{y_i&gt;f(\mathbf{x}_i)}
w_i(y_i-f(\mathbf{x}_i))\right. +\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td><span class="math inline">\(\left.(1-\alpha)\sum_{y_i\leq
f(\mathbf{x}_i)} w_i(f(\mathbf{x}_i)-y_i)\right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Initial value</td>
<td><span class="math inline">\(\mathrm{quantile}^{(\alpha)}_w(y)\)</span></td>
</tr>
<tr class="even">
<td align="left">Gradient</td>
<td><span class="math inline">\(z_i=\alpha
I(y_i&gt;f(\mathbf{x}_i))-(1-\alpha)I(y_i\leq
f(\mathbf{x}_i))\)</span></td>
</tr>
<tr class="odd">
<td align="left">Terminal node estimates</td>
<td><span class="math inline">\(\mathrm{quantile}^{(\alpha)}_w(z)\)</span></td>
</tr>
</tbody>
</table>
<p>Notes: <span class="math inline">\(\mathrm{quantile}^{(\alpha)}_w(y)\)</span> denotes
the weighted quantile, defined as the solution, <em>q</em>, to the
equation <span class="math inline">\(\sum w_iI(y_i\leq q)=\alpha\sum
w_i\)</span></p>
</div>
<div id="cox-proportional-hazard" class="section level2">
<h2>Cox Proportional Hazard</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td align="left"><span class="math inline">\(-2\sum
w_i(\delta_i(f(\mathbf{x}_i)-\log(R_i/w_i)))\)</span></td>
</tr>
<tr class="even">
<td align="left">Initial value</td>
<td align="left">0</td>
</tr>
<tr class="odd">
<td align="left">Gradient</td>
<td align="left"><span class="math inline">\(\displaystyle z_i=\delta_i
- \sum_j \delta_j \frac{w_jI(t_i\geq t_j)e^{f(\mathbf{x}_i)}} {\sum_k
w_kI(t_k\geq t_j)e^{f(\mathbf{x}_k)}}\)</span></td>
</tr>
<tr class="even">
<td align="left">Terminal node estimates</td>
<td align="left">Newton-Raphson algorithm</td>
</tr>
</tbody>
</table>
<ol style="list-style-type: decimal">
<li><p>Initialize the terminal node predictions to 0, <span class="math inline">\(\mathbf{\rho}=0\)</span></p></li>
<li><p>Let <span class="math inline">\(\displaystyle
p_i^{(k)}=\frac{\sum_j I(k(j)=k)I(t_j\geq
t_i)e^{f(\mathbf{x}_i)+\rho_k}} {\sum_j I(t_j\geq
t_i)e^{f(\mathbf{x}_i)+\rho_k}}\)</span></p></li>
<li><p>Let <span class="math inline">\(g_k=\sum
w_i\delta_i\left(I(k(i)=k)-p_i^{(k)}\right)\)</span></p></li>
<li><p>Let <span class="math inline">\(\mathbf{H}\)</span> be a <span class="math inline">\(k\times k\)</span> matrix with diagonal
elements</p>
<ul>
<li><p>Set diagonal elements <span class="math inline">\(H_{mm}=\sum
w_i\delta_i p_i^{(m)}\left(1-p_i^{(m)}\right)\)</span></p></li>
<li><p>Set off diagonal elements <span class="math inline">\(H_{mn}=-\sum w_i\delta_i
p_i^{(m)}p_i^{(n)}\)</span></p></li>
</ul></li>
<li><p>Newton-Raphson update <span class="math inline">\(\mathbf{\rho}
\leftarrow \mathbf{\rho} - \mathbf{H}^{-1}\mathbf{g}\)</span></p></li>
<li><p>Return to step 2 until convergence</p></li>
</ol>
<p>Notes:</p>
<ul>
<li><p><span class="math inline">\(t_i\)</span> is the survival time and
<span class="math inline">\(\delta_i\)</span> is the death
indicator.</p></li>
<li><p><span class="math inline">\(R_i\)</span> denotes the hazard for
the risk set, <span class="math inline">\(R_i=\sum_{j=1}^N w_jI(t_j\geq
t_i)e^{f(\mathbf{x}_i)}\)</span></p></li>
<li><p><span class="math inline">\(k(i)\)</span> indexes the terminal
node of observation <span class="math inline">\(i\)</span></p></li>
<li><p>For speed, <code>gbm()</code> does only one step of the
Newton-Raphson algorithm rather than iterating to convergence. No
appreciable loss of accuracy since the next boosting iteration will
simply correct for the prior iterations inadequacy.</p></li>
<li><p><code>gbm()</code> initially sorts the data by survival time.
Doing this reduces the computation of the risk set from <span class="math inline">\(O(n^2)\)</span> to <span class="math inline">\(O(n)\)</span> at the cost of a single up front
sort on survival time. After the model is fit, the data are then put
back in their original order.</p></li>
</ul>
</div>
<div id="poisson" class="section level2">
<h2>Poisson</h2>
<table>
<tbody>
<tr class="odd">
<td align="left">Deviance</td>
<td>-2<span class="math inline">\(\frac{1}{\sum w_i} \sum
w_i(y_if(\mathbf{x}_i)-\exp(f(\mathbf{x}_i)))\)</span></td>
</tr>
<tr class="even">
<td align="left">Initial value</td>
<td><span class="math inline">\(\displaystyle f(\mathbf{x})=
\log\left(\frac{\sum w_iy_i}{\sum w_ie^{o_i}}\right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Gradient</td>
<td><span class="math inline">\(z_i=y_i -
\exp(f(\mathbf{x}_i))\)</span></td>
</tr>
<tr class="even">
<td align="left">Terminal node estimates</td>
<td><span class="math inline">\(\displaystyle \log\frac{\sum
w_iy_i}{\sum w_i\exp(f(\mathbf{x}_i))}\)</span></td>
</tr>
</tbody>
</table>
<p>The Poisson class includes special safeguards so that the most
extreme predicted values are <span class="math inline">\(e^{-19}\)</span> and <span class="math inline">\(e^{+19}\)</span>. This behavior is consistent with
<code>glm()</code>.</p>
</div>
<div id="pairwise" class="section level2">
<h2>Pairwise</h2>
<p>This distribution implements ranking measures following the
<em>LambdaMart</em> algorithm Burges (2010). Instances belong to
<em>groups</em>; all pairs of items with different labels, belonging to
the same group, are used for training. In <em>Information Retrieval</em>
applications, groups correspond to user queries, and items to (feature
vectors of) documents in the associated match set to be ranked.</p>
<p>For consistency with typical usage, our goal is to <em>maximize</em>
one of the <em>utility</em> functions listed below. Consider a group
with instances <span class="math inline">\(x_1, \dots, x_n\)</span>,
ordered such that <span class="math inline">\(f(x_1) \geq f(x_2) \geq
\dots \geq f(x_n)\)</span>; i.e., the <em>rank</em> of <span class="math inline">\(x_i\)</span> is <span class="math inline">\(i\)</span>, where smaller ranks are preferable.
Let <span class="math inline">\(P\)</span> be the set of all ordered
pairs such that <span class="math inline">\(y_i &gt; y_j\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Concordance:</strong> Fraction of concordant (i.e,
correctly ordered) pairs. For the special case of binary labels, this is
equivalent to the Area under the ROC Curve. <span class="math display">\[\left\{ \begin{array}{l l}\frac{\|\{(i,j)\in P |
   f(x_i)&gt;f(x_j)\}\|}{\|P\|}
&amp; P \neq \emptyset\\
0 &amp; \mbox{otherwise.}
\end{array}\right.
\]</span></p></li>
<li><p><strong>MRR:</strong> Mean reciprocal rank of the highest-ranked
positive instance (it is assumed <span class="math inline">\(y_i\in\{0,1\}\)</span>): <span class="math display">\[\left\{ \begin{array}{l l}\frac{1}{\min\{1 \leq i
\leq n |y_i=1\}}
&amp; \exists i: \, 1 \leq i \leq n, y_i=1\\
0 &amp; \mbox{otherwise.}\end{array}\right.\]</span></p></li>
<li><p><strong>MAP:</strong> Mean average precision, a generalization of
MRR to multiple positive instances: <span class="math display">\[\left\{
\begin{array}{l l} \frac{\sum_{1\leq i\leq n | y_i=1} \|\{1\leq j\leq i
|y_j=1\}\|\,/\,i}{\|\{1\leq i\leq n | y_i=1\}\|}  &amp; \exists i: \,
1 \leq i \leq n, y_i=1\\
0 &amp; \mbox{otherwise.}\end{array}\right.\]</span></p></li>
<li><p><strong>nDCG:</strong> Normalized discounted cumulative gain:
<span class="math display">\[\frac{\sum_{1\leq i\leq n} \log_2(i+1) \,
y_i}{\sum_{1\leq i\leq n}
  \log_2(i+1) \, y&#39;_i},\]</span> where <span class="math inline">\(y&#39;_1, \dots, y&#39;_n\)</span> is a reordering
of <span class="math inline">\(y_1,\dots,y_n\)</span> with <span class="math inline">\(y&#39;_1 \geq y&#39;_2 \geq \dots \geq
y&#39;_n\)</span>.</p></li>
</ol>
<p>The generalization to multiple (possibly weighted) groups is
straightforward. Sometimes a cut-off rank <span class="math inline">\(k\)</span> is given for <em>MRR</em> and
<em>nDCG</em>, in which case we replace the outer index <span class="math inline">\(n\)</span> by <span class="math inline">\(\min(n,k)\)</span>.</p>
<p>The initial value for <span class="math inline">\(f(x_i)\)</span> is
always zero. We derive the gradient of a cost function whose gradient
locally approximates the gradient of the IR measure for a fixed ranking:
<span class="math display">\[\begin{aligned}
\Phi &amp; = \sum_{(i,j) \in P} \Phi_{ij}\\
&amp; = \sum_{(i,j) \in P} |\Delta Z_{ij}| \log \left( 1 + e^{-(f(x_i) -
    f(x_j))}\right),
\end{aligned}\]</span> where <span class="math inline">\(|\Delta
Z_{ij}|\)</span> is the absolute utility difference when swapping the
ranks of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, while leaving all other instances the
same. Define <span class="math display">\[\begin{aligned}
  \lambda_{ij} &amp;= \frac{\partial\Phi_{ij}}{\partial f(x_i)}\\
  &amp;= -|\Delta Z_{ij}| \frac{1}{1 + e^{f(x_i) - f(x_j)}}\\
&amp;= -|\Delta Z_{ij}| \, \rho_{ij},
\end{aligned}\]</span> with <span class="math display">\[\rho_{ij} = -
\frac{\lambda_{ij }}{|\Delta Z_{ij}|} = \frac{1}{1 + e^{f(x_i) -
f(x_j)}}\]</span></p>
<p>For the gradient of <span class="math inline">\(\Phi\)</span> with
respect to <span class="math inline">\(f(x_i)\)</span>, define <span class="math display">\[\begin{aligned}
\lambda_i &amp;= \frac{\partial \Phi}{\partial f(x_i)}\\
&amp;= \sum_{j|(i,j) \in P} \lambda_{ij} -\hspace{-0.1in}\sum_{j|(j,i)
\in P} \lambda_{ji}\\
&amp;= -\hspace{-0.1in}\sum_{j|(i,j) \in P} |\Delta Z_{ij}| \,
\rho_{ij}  + \hspace{-0.1in}\sum_{j|(j,i) \in P} |\Delta Z_{ji}| \,
\rho_{ji}.
\end{aligned}\]</span></p>
<p>The second derivative is <span class="math display">\[\begin{aligned}
   \gamma_i &amp;\stackrel{def}{=} \frac{\partial^2\Phi}{\partial
f(x_i)^2}\\
    &amp;=  \sum_{j|(i,j) \in P} |\Delta Z_{ij}| \, \rho_{ij} \,
(1-\rho_{ij})
+ \sum_{j|(j,i) \in P} |\Delta Z_{ji}| \, \rho_{ji} \, (1-\rho_{ji}).
\end{aligned}\]</span></p>
<p>Now consider again all groups with associated weights. For a given
terminal node, let <span class="math inline">\(i\)</span> range over all
contained instances. Then its estimate is <span class="math display">\[-\frac{\sum_i v_i\lambda_{i}}{\sum_i v_i
\gamma_i},\]</span> where <span class="math inline">\(v_i=w(\mathbf{group}(i))/\|\{(j,k)\in\mathbf{group}(i)\}\|.\)</span></p>
<p>In each iteration, instances are reranked according to the
preliminary scores <span class="math inline">\(f(x_i)\)</span> to
determine the <span class="math inline">\(|\Delta Z_{ij}|\)</span>. Note
that in order to avoid ranking bias, we break ties by adding a small
amount of random noise.</p>
</div>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>C. Burges (2010). “From RankNet to LambdaRank to LambdaMART: An
Overview,” <em>Microsoft Research Technical Report
MSR-TR-2010-82</em>.</p>
<p>Y. Freund and R.E. Schapire (1997). “A decision-theoretic
generalization of on-line learning and an application to boosting,”
<em>Journal of Computer and System Sciences</em> 55(1):119-139.</p>
<p>J.H. Friedman (2001). “Greedy Function Approximation: A Gradient
Boosting Machine,” <em>Annals of Statistics</em> 29(5):1189-1232.</p>
<p>J.H. Friedman (2002). “Stochastic Gradient Boosting,”
<em>Computational Statistics and Data Analysis</em> 38(4):367-378.</p>
<p>J.H. Friedman, T. Hastie, R. Tibshirani (2000). “Additive Logistic
Regression: a Statistical View of Boosting,” <em>Annals of
Statistics</em> 28(2):337-374.</p>
<p>B. Kriegler and R. Berk (2010). “Small Area Estimation of the
Homeless in Los Angeles, An Application of Cost-Sensitive Stochastic
Gradient Boosting,” <em>Annals of Applied Statistics</em>
4(3):1234-1255.</p>
<p>G. Ridgeway (1999). “The state of boosting,” <em>Computing Science
and Statistics</em> 31:172-181.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
